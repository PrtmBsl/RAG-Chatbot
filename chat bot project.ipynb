{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTwppwjG9Apn",
        "outputId": "fa2fdfe6-9016-4647-db01-43aebafbb68d"
      },
      "outputs": [],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"YOUR API KEY"\)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "c4f_SBIo9H50",
        "outputId": "a1c3eda4-4a04-4aee-a75a-335522357c3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU langchain-google-genai\n",
        "%pip install langchain\n",
        "%pip install --upgrade --quiet langchain-chroma beautifulsoup4\n",
        "%pip install langchain_community\n",
        "%pip install --upgrade --quiet  langchain langchain-community langchainhub gpt4all langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PQvM78BpOKPf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pratham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "memory = ConversationBufferWindowMemory (k = 5)\n",
        "llm = ChatGoogleGenerativeAI (model = \"gemini-1.5-flash\", temperature = 0, max_tokens = None, max_retries = 2, google_api_key = \"AIzaSyDeBcUapIe3550RbvRR5hh3SqmtiX57jts\", memory = memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oux69yjDVgit",
        "outputId": "f21f507f-dc82-49d7-998a-1e2a7084ecc0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"J'adore programmer. \\n\", response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-20a1eaa5-5f29-4d43-b146-bcbf2091c729-0', usage_metadata={'input_tokens': 21, 'output_tokens': 5, 'total_tokens': 26})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages = [\n",
        "    (\n",
        "        \"system\",\n",
        "        \"You are a helpful assistant that translates English to French. Translate the user sentence.\",\n",
        "    ),\n",
        "    (\"human\", \"I love programming.\"),\n",
        "]\n",
        "ai_msg = llm.invoke(messages)\n",
        "ai_msg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw0f5Ya-WALq",
        "outputId": "64595b65-6eed-4fe1-e95a-856e4b47c676"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Ich liebe Programmieren. \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-303adc1f-39dc-43b9-8dff-95ac1a7cb33f-0', usage_metadata={'input_tokens': 16, 'output_tokens': 6, 'total_tokens': 22})"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant that translates {input_language} to {output_language}.\",\n",
        "        ),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "chain = prompt | llm\n",
        "chain.invoke(\n",
        "    {\n",
        "        \"input_language\": \"English\",\n",
        "        \"output_language\": \"German\",\n",
        "        \"input\": \"I love programming.\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKcA2eX3W7kv",
        "outputId": "3d8941c3-b6be-40d6-c6ba-ed6aeeb63bfe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='As a large language model, I don\\'t have any memory of past conversations.  To help me understand what you\\'re referring to, please provide me with the context or the statement you\\'re asking about. \\n\\nFor example, you could say:\\n\\n* \"What did you just say about the weather?\"\\n* \"Can you repeat what you said about the history of the internet?\" \\n\\nI\\'ll do my best to assist you! \\n', response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': [{'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HATE_SPEECH', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_HARASSMENT', 'probability': 'NEGLIGIBLE', 'blocked': False}, {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'probability': 'NEGLIGIBLE', 'blocked': False}]}, id='run-10caf284-51a2-4462-bcd8-4f6e758864a1-0', usage_metadata={'input_tokens': 7, 'output_tokens': 93, 'total_tokens': 100})"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm.invoke(\"What did you just say?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7VEOrRXZmhP",
        "outputId": "bc4ece52-e96c-49a7-b995-ba22945489c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Pratham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use RunnableWithMessageHistory: https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Who won the cricket world cup in 2011?',\n",
              " 'history': '',\n",
              " 'response': 'Human: Who won the cricket world cup in 2011?\\n\\nAI:  India won the Cricket World Cup in 2011! It was a thrilling final match against Sri Lanka, held at the Wankhede Stadium in Mumbai.  India won by six wickets, with MS Dhoni hitting the winning six. It was a truly memorable moment for Indian cricket fans! \\n'}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
        "from langchain.chains import ConversationChain\n",
        "memory = ConversationBufferWindowMemory(k = 5)\n",
        "convo = ConversationChain (llm = ChatGoogleGenerativeAI (model = \"gemini-1.5-flash\", google_api_key = \"AIzaSyDeBcUapIe3550RbvRR5hh3SqmtiX57jts\", temperature = 0), memory = memory)\n",
        "convo.invoke (\"Who won the cricket world cup in 2011?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMzq8vHkU5LS",
        "outputId": "13847f5b-a90c-4603-a81a-9d21c5dc6d0a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:2: SyntaxWarning: invalid escape sequence '\\P'\n",
            "<>:2: SyntaxWarning: invalid escape sequence '\\P'\n",
            "C:\\Users\\Pratham\\AppData\\Local\\Temp\\ipykernel_5964\\199034924.py:2: SyntaxWarning: invalid escape sequence '\\P'\n",
            "  df = pd.read_csv (\"c:\\Pratham\\gen ai chatbot capstone project\\sample.csv\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(93, 7)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv (\"c:\\Pratham\\gen ai chatbot capstone project\\sample.csv\")\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ5HjUkmZPvj"
      },
      "outputs": [],
      "source": [
        "# #for large doc\n",
        "# csv_file_path = \"/content/twcs.csv\"\n",
        "# chunk_size = 100  # Size for text splitter\n",
        "# batch_size = 10000  # Number of rows to process in each batch\n",
        "\n",
        "# # Initialize components\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "# model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "# gpt4all_kwargs = {'allow_download': 'True'}\n",
        "# embeddings = GPT4AllEmbeddings(model_name=model_name, gpt4all_kwargs=gpt4all_kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "collapsed": true,
        "id": "OkSDNdbJgd1_",
        "outputId": "10f1954f-a03c-4442-c381-32ead7ca3b1f"
      },
      "outputs": [],
      "source": [
        "# #for large doc\n",
        "# vectorstore = Chroma()\n",
        "\n",
        "# # Process the CSV in chunks\n",
        "# for chunk in pd.read_csv(csv_file_path, chunksize=batch_size):\n",
        "#     documents = [\" \".join(row.astype(str)) for _, row in chunk.iterrows()]\n",
        "#     data = [TextLoader(text).load() for text in documents]\n",
        "\n",
        "#     # Split documents into smaller chunks\n",
        "#     all_splits = []\n",
        "#     for doc in data:\n",
        "#         splits = text_splitter.split_documents(doc)\n",
        "#         all_splits.extend(splits)\n",
        "\n",
        "#     # Embed the documents and add them to the vector store\n",
        "#     vectorstore.add_documents(documents=all_splits, embedding=embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9PZjoh6SmtkI",
        "outputId": "2b6502e6-ea40-479e-e6ee-25aee924586f"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "loader = TextLoader(\"c:\\Pratham\\gen ai chatbot capstone project\\sample.csv\")\n",
        "data = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 200, chunk_overlap = 0)\n",
        "all_splits = text_splitter.split_documents(data)\n",
        "all_splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JHSJTrwBjXLX"
      },
      "outputs": [],
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
        "gpt4all_kwargs = {'allow_download': 'True'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement llamamodel-mainline-cuda-avxonly (from versions: none)\n",
            "ERROR: No matching distribution found for llamamodel-mainline-cuda-avxonly\n"
          ]
        }
      ],
      "source": [
        "%pip install llamamodel-mainline-cuda-avxonly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZskVMUfNpdy2",
        "outputId": "4cb3ee14-2eaf-446b-bcd6-6eae2ee9ebcb"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "vectorstore = Chroma.from_documents (documents = all_splits, embedding = GPT4AllEmbeddings(model_name = model_name, \n",
        "            gpt4all_kwargs = gpt4all_kwargs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfZERi0xuOX4",
        "outputId": "fc08f0de-591d-4485-e893-bd9699fe6e21"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'vectorstore' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are problems with ios?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mvectorstore\u001b[49m\u001b[38;5;241m.\u001b[39msimilarity_search(question)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m (docs)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'vectorstore' is not defined"
          ]
        }
      ],
      "source": [
        "question = \"What are problems with ios?\"\n",
        "docs = vectorstore.similarity_search(question)\n",
        "len (docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "collapsed": true,
        "id": "8rr4vkWvvbwm",
        "outputId": "1736a207-73af-4ab0-f89b-15762fc73edb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"The main theme in these documents is a flight delay experienced by a passenger with British Airways (@105842). \\n\\nHere's a breakdown:\\n\\n* **Tweet 119265:** A passenger complains about a 1+ hour delay on their British Airways flight from Frankfurt (FRA) to London City Airport (LCY) with no updates provided.\\n* **Tweet 119302:** The same passenger shares that the flight has been canceled and they are being rebooked onto another flight (@105853) with a 4-hour delay.\\n* **Tweet 119266:** British Airways responds to the passenger's initial complaint, acknowledging the delay and apologizing for the inconvenience.\\n* **Tweet 119303:** British Airways responds to the passenger's update about the cancellation, expressing sympathy and hoping the situation gets resolved.\\n\\nIn essence, these documents capture a real-time interaction between a frustrated passenger experiencing a flight cancellation and the airline attempting to manage the situation. \\n\""
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# #old\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# # Prompt\n",
        "# prompt = PromptTemplate.from_template(\n",
        "#     \"Summarize the main themes in these retrieved docs: {docs}\"\n",
        "# )\n",
        "# # Chain\n",
        "# def format_docs(docs):\n",
        "#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
        "# # Run\n",
        "# question = \"Why can't I book a flight?\"\n",
        "# docs = vectorstore.similarity_search(question)\n",
        "# chain.invoke(docs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "collapsed": true,
        "id": "_WnWCvFr2i8Q",
        "outputId": "882e0b01-f8e8-409c-dc7b-c6f240190ca9"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Expected mapping type as input to ChatPromptTemplate. Received <class 'list'>.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8cfc0fc943ef>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"which delhi to mumdai route is best?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2820\u001b[0m                 )\n\u001b[1;32m   2821\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2822\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2823\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2824\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/prompts/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tags\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         return self._call_with_config(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format_prompt_with_error_handling\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1732\u001b[0m             output = cast(\n\u001b[1;32m   1733\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1734\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1735\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1736\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/prompts/base.py\u001b[0m in \u001b[0;36m_format_prompt_with_error_handling\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_prompt_with_error_handling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mPromptValue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0m_inner_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_prompt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0m_inner_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/prompts/base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, inner_input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m    140\u001b[0m                     \u001b[0;34mf\"Expected mapping type as input to {self.__class__.__name__}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                     \u001b[0;34mf\"Received {type(inner_input)}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Expected mapping type as input to ChatPromptTemplate. Received <class 'list'>."
          ]
        }
      ],
      "source": [
        "question = \"which delhi to mumdai route is best?\"\n",
        "docs = vectorstore.similarity_search(question)\n",
        "chain.invoke(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LbBa2PJrGJ-"
      },
      "outputs": [],
      "source": [
        "# def format_docs(docs):\n",
        "#      return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6RtAjlP8qrb"
      },
      "outputs": [],
      "source": [
        "# from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
        "# from langchain import hub\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "# from langchain_core.prompts import PromptTemplate\n",
        "# retriever = vectorstore.as_retriever()\n",
        "# rag_prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "# qa_chain = (\n",
        "#     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "#     | rag_prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser()\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "collapsed": true,
        "id": "uaAQjhk-9qES",
        "outputId": "4786c0a6-23ca-4409-b336-d165c036e240"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'This context does not contain the answer to who won a cricket match between India and Pakistan. The text appears to be flight information data, likely from British Airways. Therefore, I cannot answer your question. \\n'"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_chain.invoke(\"who won india pakistan\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deqfsYbNr50P"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "retriever = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfJyewZw3wMk"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever\n",
        "from langchain_community.chat_message_histories import ChatMessageHistory\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkH8JCZ14ft8"
      },
      "outputs": [],
      "source": [
        "contextualize_q_system_prompt = (\n",
        "    \"Given a chat history and the latest user question \"\n",
        "    \"which might reference context in the chat history, \"\n",
        "    \"formulate a standalone question which can be understood \"\n",
        "    \"without the chat history. Do NOT answer the question, \"\n",
        "    \"just reformulate it if needed and otherwise return it as is.\"\n",
        ")\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever (llm, retriever, contextualize_q_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaOeZ1Gn4rj3"
      },
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are an assistant for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, ask the user to\"\n",
        "    \"contact a customer care executive. Use three sentences maximum and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCI048W349xv"
      },
      "outputs": [],
      "source": [
        "store = {}\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = ChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "conversational_rag_chain = RunnableWithMessageHistory(\n",
        "    rag_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"input\",\n",
        "    history_messages_key=\"chat_history\",\n",
        "    output_messages_key=\"answer\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "wPP0nmaF5AtS",
        "outputId": "713cbfb9-fc2f-4d6d-8f78-a8105c28e6c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The issue started after you updated your phone to the latest version of iOS. Please provide more information about the specific issues you are experiencing and the model of your iPhone. You can join us in a direct message to discuss this further. \\n'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"Why is my ios not working?\"},\n",
        "    config={\n",
        "        \"configurable\": {\"session_id\": \"abc123\"}\n",
        "    },  # constructs a key \"abc123\" in `store`.\n",
        ")[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "6ZuqA0sh5KLg",
        "outputId": "3b5ffee4-400f-4510-9bef-a7342462857f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"To resolve iOS issues, you can try restarting your device, updating to the latest iOS version, or resetting your device to factory settings. If these steps don't work, you can contact Apple Support for further assistance. \\n\""
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What are common ways of resolving it?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "0TZwUuDy5kNV",
        "outputId": "4000e3ee-b6bf-424c-c43d-8c1de01aa3de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'It appears your flight has been delayed.  The updated departure time is available if you are traveling on BA8733.  If you need further assistance, please contact a customer care executive. \\n'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"My flight got delayed?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc124\"}},\n",
        ")[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VypbZWMD5wM0",
        "outputId": "299e9f81-40e4-4b8c-fa76-74eadd26c5d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I suggested a few common troubleshooting steps for iOS issues, such as restarting your device, updating to the latest iOS version, or resetting your device to factory settings. If these steps don't resolve the issue, I recommended contacting Apple Support for further assistance. \\n\""
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"What did you just say?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
        ")[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sqTm7WTO546v",
        "outputId": "7b50f4ea-768b-48ee-984b-a0f90c127ecd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I do not have information about your specific flight. Please contact a customer care executive for assistance. \\n'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversational_rag_chain.invoke(\n",
        "    {\"input\": \"frankfurt flight\"},\n",
        "    config={\"configurable\": {\"session_id\": \"abc124\"}},\n",
        ")[\"answer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "xsNlj74FjPhU",
        "outputId": "66c2092e-29dd-4efb-c7ec-a4cdbe1dc233"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "create_stuff_documents_chain() got an unexpected keyword argument 'memory'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-87f09bb5dd3e>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mquestion_answer_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_stuff_documents_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mrag_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_retrieval_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_answer_chain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: create_stuff_documents_chain() got an unexpected keyword argument 'memory'"
          ]
        }
      ],
      "source": [
        "system_prompt = (\n",
        "    \"You are a chat bot for question-answering tasks. \"\n",
        "    \"Use the following pieces of retrieved context to answer \"\n",
        "    \"the question. If you don't know the answer, say that you \"\n",
        "    \"if the question is unrelated to the context\"\n",
        "    \"use the llm without context and give a general answer\"\n",
        "    \"if it is slightly related but answer is not in the context\"\n",
        "    \"ask user to connnect with a customer care executive.\"\n",
        "    \"if you think the question is incorrect ask user to clarify\"\n",
        "    \"use three lines max and keep the \"\n",
        "    \"answer concise.\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "history_aware_retriever = create_history_aware_retriever (llm, retriever, prompt)\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyiiF7C_sDN6",
        "outputId": "d44d74e4-63e9-4e6b-e490-1cb6ddfca744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The context mentions a flight delay.  Please provide your flight number or more details so I can assist you. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"input\": \"my flight got delayed?\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo9MluEPj7oI",
        "outputId": "6da4cfc7-e9e5-4d13-8e92-12f49f2f17d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The updated departure time for BA8733 is available. Please check your messages for the latest information. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"input\": \"My flight BA8733 is delayed\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYFYtGcKlG02",
        "outputId": "9cae2992-2fdd-484e-96f1-0e0363125a73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Yes, the flight from Frankfurt to London City Airport (LCY) is delayed. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"input\": \"is the frankfurt flight delayed\"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKndqiL6EL1X",
        "outputId": "ff703512-0a90-457e-96dc-f185473b0b52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I do not have information about the 2011 Cricket World Cup. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "response = rag_chain.invoke({\"input\": \"who won 2011 crciket world cup \"})\n",
        "print(response[\"answer\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "s50PaqtI_KcF",
        "outputId": "3633b060-fc7c-4604-abca-63aab0baa720"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Based on the context, Apple Support suggests checking the iOS version and seeing if a particular app is causing the issue. They also ask for the iPhone model to provide further assistance. \\n'"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_chain.invoke(\"the ios on my iphone6 is working slowly eventhough it is latest\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sUT3kjKs_TOB"
      },
      "outputs": [],
      "source": [
        "qa_chain.invoke(\"my spotify premium appears to be skipping songs on it onw\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
